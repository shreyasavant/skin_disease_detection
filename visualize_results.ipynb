{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Results Visualization\n",
        "\n",
        "This notebook loads a trained model and visualizes its performance on the test dataset.\n",
        "\n",
        "**Setup:**\n",
        "1. Update the paths below to point to your trained model and dataset\n",
        "2. Run all cells to see visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "MODEL_PATH = \"models/cnn_model.keras\"  # specify path to model\n",
        "DATASET_DIR = \"dataset\"  # specify path to dataset directory\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Model and Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Loading model from: {MODEL_PATH}\")\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test dataset\n",
        "test_dir = Path(DATASET_DIR) / \"test\"\n",
        "print(f\"Loading test dataset from: {test_dir}\")\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "class_names = test_ds.class_names\n",
        "print(f\"\\nFound {len(class_names)} classes: {class_names}\")\n",
        "\n",
        "# Collect all images and labels\n",
        "test_images = []\n",
        "test_labels = []\n",
        "\n",
        "for images, labels in test_ds:\n",
        "    test_images.append(images.numpy())\n",
        "    test_labels.append(labels.numpy())\n",
        "\n",
        "test_images = np.concatenate(test_images, axis=0)\n",
        "test_labels = np.concatenate(test_labels, axis=0)\n",
        "\n",
        "print(f\"Test set size: {len(test_labels)} images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Make Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Making predictions...\")\n",
        "y_pred_prob = model.predict(test_images, verbose=1)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "print(f\"\\nPredictions complete. Shape: {y_pred_prob.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Overall Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc = accuracy_score(test_labels, y_pred)\n",
        "prec = precision_score(test_labels, y_pred, average=\"macro\", zero_division=0)\n",
        "rec = recall_score(test_labels, y_pred, average=\"macro\", zero_division=0)\n",
        "f1 = f1_score(test_labels, y_pred, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(\"Overall Performance Metrics:\")\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "\n",
        "# Visualize metrics\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
        "values = [acc, prec, rec, f1]\n",
        "bars = ax.bar(metrics, values, color=[\"#3498db\", \"#2ecc71\", \"#e74c3c\", \"#f39c12\"])\n",
        "ax.set_ylim([0, 1])\n",
        "ax.set_ylabel(\"Score\", fontsize=12)\n",
        "ax.set_title(\"Overall Model Performance\", fontsize=14, fontweight=\"bold\")\n",
        "for bar, val in zip(bars, values):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "            f\"{val:.3f}\", ha=\"center\", va=\"bottom\", fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(test_labels, y_pred)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=class_names,\n",
        "    yticklabels=class_names,\n",
        "    cbar_kws={\"label\": \"Count\"}\n",
        ")\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix\", fontsize=14, fontweight=\"bold\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Normalized confusion matrix\n",
        "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(\n",
        "    cm_normalized,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=class_names,\n",
        "    yticklabels=class_names,\n",
        "    cbar_kws={\"label\": \"Proportion\"}\n",
        ")\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.title(\"Normalized Confusion Matrix\", fontsize=14, fontweight=\"bold\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report = classification_report(test_labels, y_pred, target_names=class_names, output_dict=True)\n",
        "\n",
        "print(\"Detailed Classification Report:\")\n",
        "print(classification_report(test_labels, y_pred, target_names=class_names))\n",
        "\n",
        "# Extract per-class metrics\n",
        "classes = [c for c in class_names if c in report]\n",
        "precisions = [report[c][\"precision\"] for c in classes]\n",
        "recalls = [report[c][\"recall\"] for c in classes]\n",
        "f1_scores = [report[c][\"f1-score\"] for c in classes]\n",
        "\n",
        "# Visualize per-class metrics\n",
        "x = np.arange(len(classes))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "ax.bar(x - width, precisions, width, label=\"Precision\", color=\"#3498db\")\n",
        "ax.bar(x, recalls, width, label=\"Recall\", color=\"#2ecc71\")\n",
        "ax.bar(x + width, f1_scores, width, label=\"F1-Score\", color=\"#e74c3c\")\n",
        "\n",
        "ax.set_xlabel(\"Class\", fontsize=12)\n",
        "ax.set_ylabel(\"Score\", fontsize=12)\n",
        "ax.set_title(\"Per-Class Performance Metrics\", fontsize=14, fontweight=\"bold\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(classes, rotation=45, ha=\"right\")\n",
        "ax.set_ylim([0, 1])\n",
        "ax.legend()\n",
        "ax.grid(axis=\"y\", alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Sample Predictions Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(images, true_labels, pred_labels, pred_probs, class_names, num_samples=16, title=\"Predictions\"):\n",
        "    \"\"\"Visualize a grid of images with their predictions.\"\"\"\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    indices = np.random.choice(len(images), min(num_samples, len(images)), replace=False)\n",
        "    \n",
        "    for i, idx in enumerate(indices):\n",
        "        img = images[idx]\n",
        "        true_label = class_names[true_labels[idx]]\n",
        "        pred_label = class_names[pred_labels[idx]]\n",
        "        conf = pred_probs[idx][pred_labels[idx]]\n",
        "        \n",
        "        axes[i].imshow(img.astype(\"uint8\"))\n",
        "        axes[i].axis(\"off\")\n",
        "        \n",
        "        color = \"green\" if true_label == pred_label else \"red\"\n",
        "        axes[i].set_title(\n",
        "            f\"True: {true_label}\\nPred: {pred_label} ({conf:.2f})\",\n",
        "            color=color,\n",
        "            fontsize=10\n",
        "        )\n",
        "    \n",
        "    plt.suptitle(title, fontsize=16, fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show random samples\n",
        "visualize_predictions(test_images, test_labels, y_pred, y_pred_prob, class_names, num_samples=16, title=\"Random Sample Predictions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Correct vs Incorrect Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate correct and incorrect predictions\n",
        "correct_mask = test_labels == y_pred\n",
        "incorrect_mask = ~correct_mask\n",
        "\n",
        "correct_images = test_images[correct_mask]\n",
        "correct_true = test_labels[correct_mask]\n",
        "correct_pred = y_pred[correct_mask]\n",
        "correct_probs = y_pred_prob[correct_mask]\n",
        "\n",
        "incorrect_images = test_images[incorrect_mask]\n",
        "incorrect_true = test_labels[incorrect_mask]\n",
        "incorrect_pred = y_pred[incorrect_mask]\n",
        "incorrect_probs = y_pred_prob[incorrect_mask]\n",
        "\n",
        "print(f\"Correct predictions: {len(correct_images)} ({len(correct_images)/len(test_labels)*100:.1f}%)\")\n",
        "print(f\"Incorrect predictions: {len(incorrect_images)} ({len(incorrect_images)/len(test_labels)*100:.1f}%)\")\n",
        "\n",
        "# Show correct predictions\n",
        "if len(correct_images) > 0:\n",
        "    visualize_predictions(\n",
        "        correct_images, correct_true, correct_pred, correct_probs, class_names,\n",
        "        num_samples=min(16, len(correct_images)),\n",
        "        title=\"Correct Predictions (Green)\"\n",
        "    )\n",
        "\n",
        "# Show incorrect predictions\n",
        "if len(incorrect_images) > 0:\n",
        "    visualize_predictions(\n",
        "        incorrect_images, incorrect_true, incorrect_pred, incorrect_probs, class_names,\n",
        "        num_samples=min(16, len(incorrect_images)),\n",
        "        title=\"Incorrect Predictions (Red)\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Confidence Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get confidence scores for predictions\n",
        "confidence_scores = np.max(y_pred_prob, axis=1)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram of confidence scores\n",
        "axes[0].hist(confidence_scores, bins=30, edgecolor=\"black\", alpha=0.7)\n",
        "axes[0].set_xlabel(\"Confidence Score\", fontsize=12)\n",
        "axes[0].set_ylabel(\"Frequency\", fontsize=12)\n",
        "axes[0].set_title(\"Distribution of Prediction Confidence\", fontsize=12, fontweight=\"bold\")\n",
        "axes[0].axvline(np.mean(confidence_scores), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(confidence_scores):.3f}\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Confidence by correctness\n",
        "axes[1].hist(confidence_scores[correct_mask], bins=30, alpha=0.6, label=\"Correct\", color=\"green\", edgecolor=\"black\")\n",
        "axes[1].hist(confidence_scores[incorrect_mask], bins=30, alpha=0.6, label=\"Incorrect\", color=\"red\", edgecolor=\"black\")\n",
        "axes[1].set_xlabel(\"Confidence Score\", fontsize=12)\n",
        "axes[1].set_ylabel(\"Frequency\", fontsize=12)\n",
        "axes[1].set_title(\"Confidence: Correct vs Incorrect\", fontsize=12, fontweight=\"bold\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average confidence for correct predictions: {np.mean(confidence_scores[correct_mask]):.3f}\")\n",
        "print(f\"Average confidence for incorrect predictions: {np.mean(confidence_scores[incorrect_mask]):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Class Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count true and predicted class distributions\n",
        "unique_true, counts_true = np.unique(test_labels, return_counts=True)\n",
        "unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# True distribution\n",
        "axes[0].bar([class_names[i] for i in unique_true], counts_true, color=\"#3498db\", edgecolor=\"black\")\n",
        "axes[0].set_xlabel(\"Class\", fontsize=12)\n",
        "axes[0].set_ylabel(\"Count\", fontsize=12)\n",
        "axes[0].set_title(\"True Class Distribution\", fontsize=12, fontweight=\"bold\")\n",
        "axes[0].tick_params(axis=\"x\", rotation=45)\n",
        "axes[0].grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "# Predicted distribution\n",
        "pred_names = [class_names[i] for i in unique_pred]\n",
        "axes[1].bar(pred_names, counts_pred, color=\"#2ecc71\", edgecolor=\"black\")\n",
        "axes[1].set_xlabel(\"Class\", fontsize=12)\n",
        "axes[1].set_ylabel(\"Count\", fontsize=12)\n",
        "axes[1].set_title(\"Predicted Class Distribution\", fontsize=12, fontweight=\"bold\")\n",
        "axes[1].tick_params(axis=\"x\", rotation=45)\n",
        "axes[1].grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
